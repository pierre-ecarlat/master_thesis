\section*{Résumé}
\addcontentsline{toc}{section}{Résumé}

Ce mémoire s'appuie en grande partie sur les travaux de recherche réalisés par l'étudiant Pierre Ecarlat lors de son stage de six mois, effectué au \textit{National Institute of Informatics} (NII), à Tokyo. Celui-ci se focalise notamment sur les différentes méthodes de \textit{Deep Learning}, et plus particulièrement sur celles utilisées en \textit{Computer Vision}, ou "vision artificielle". Ce domaine a de nombreuses applications possibles (reconnaissance et classification d'images,~\cite{RUSS15, LECU98} tracking d'objets dans un flux vidéo~\cite{WANG13, ROSS08}, segmentation détaillée d'une image et extraction de son contexte~\cite{LONG15, BADR15}, détection de caractéristiques indiscernables pour l'oeil humain~\cite{SILV16, MNIH13}, ...), et constitue un champ de recherche très actif ces dernières années.

Dans le cadre de ce projet, notre objectif était d'apprendre à un algorithme de \textit{Deep Learning} à segmenter sémantiquement une image aérienne (capturée par un drone). Cela signifie être capable de déterminer, pour chaque pixel de cette vue aérienne, ce que celui-ci représente (s'il s'agit d'un pixel d'arbre, de rue, de maison...). Un algorithme de ce type permettrait notamment à un drone de savoir presque en temps réel ce qui se trouve sous lui et ainsi de déterminer, par exemple, s'il est en mesure de se poser au sol ou non, ou s'il est face à une situation particulière (notamment en cas de mission de sauvetage pour détecter un feu, un corps, ...). Une autre application, plus ambitieuse, pourrait être un service de livraison aérien intelligent, ou la création de cartes dynamiques qui s'actualiseraient en temps réel.

Ces six mois ont donc servi à déterminer une méthode d'apprentissage qui soit, d'un côté, efficace pour des bases de données de prises de vues aériennes et, également, suffisamment rapide pour pouvoir être utilisée presque en temps réel. De plus, nous voulions également étudier la flexibilité de notre architecture : s'assurer que celle-ci soit capable d'étendre son apprentissage à d'autres types d'images (vues aériennes prises sous un autre point de vue, images présentant des incidents rares (incendie, éboulement, ...) ou encore des images prises depuis différentes caméras avec des résolutions distinctes).

En résumé, le projet s'ancre autour de la problématique suivante : \\
\textit{"Construire une architecture de \textit{Deep Learning} capable de segmenter sémantiquement, aussi rapidement que possible, des prises de vues aériennes tout en étudiant sa capacité d'adaptation face à de nouvelles situations."}

Ce mémoire présentera donc tout d'abord un état de l'art sur les différentes méthodes de \textit{Deep Learning} tout en expliquant leur fonctionnement, en se concentrant sur les modèles utilisés en vision artificielle. Nous ne tirerons finalement de cet état de l'art que deux architectures de segmentation pouvant potentiellement être réutilisées efficacement sur des prises de vues aériennes : SegNet~\cite{BADR15} et FCN (et ses variantes)~\cite{LONG15}. De plus, nous retenons également l'architecture de classification ResNet~\cite{HE16}, et y avons ajouté la méthode d'\textit{upsampling} de FCN-32 (cette architecture sera appelée FCN-32-ResNet), de façon à effectuer de la segmentation. Nous avons également décidé de créer une variante consistant à utiliser la méthode d'\textit{upsampling} de FCN-16, visant à altérer d'autant plus les paramètres d'apprentissage de notre architecture, suivant la méthode de Long et al.~\cite{LONG15}, en ajoutant une connexion supplémentaire à la fin de notre architecture. Ce dernier modèle sera appelé FCN-16-ResNet pour la suite du projet. À notre connaissance, ces deux dernières architectures hybrides n'ont encore jamais été testées dans la littérature.

Nous nous intéresserons également au problème de la base de données d'images à utiliser. Effectivement, pour apprendre à segmenter des images avec précision, un modèle de \textit{Deep Learning} doit baser son apprentissage sur des données que nous lui fournissons, et la composition de celles-ci peut avoir une très grande influence sur les résultats. Ainsi, après avoir étudié les principales bases de données existantes, ainsi que leurs principales caractéristiques, nous avons défini les deux bases que nous utiliserons pour la suite des expériences :
\begin{itemize}
\item \textbf{Swiss dataset :} basée sur les images capturées par l'entreprise senseFly\footnote{sensefly.com/drones/example-datasets}. Elle est constituée de 100 grandes images (4068x3456), et de leur segmentation associée.
\item \textbf{Okutama dataset :} basée sur des images capturées par un de nos drones, lors de ses vols au-dessus de la ville d'Okutama, au Japon. Elle est constituée de 28 images (3840x2160), et de leur segmentation associée.
\end{itemize}
Toutes deux se basent sur des prises de vues orientées à la perpendiculaire vers le sol, et segmentent leurs images en 10 classes distinctes d'assez haut niveau (bâtiment, sol pavé, voiture, ...). En revanche, elles se basent sur des lieux géographiquement très éloignés, et donc significativement différents, ce qui nous permettra de tester la flexibilité de l'apprentissage entre nos deux bases de données, et de tester différentes méthodes de \textit{Deep Transfer Learning} (transfert d'un apprentissage à un autre, peu importe leurs sources).

Nos expériences se diviseront donc en cinq grandes parties, définies en partie 3 et étudiées en partie 4. \\
\begin{enumerate}
\item Les images de nos bases de données étant considérablement grandes, nous devons tout d'abord trouver le meilleur moyen de réduire  leur taille avant toute autre expérience, de façon à éviter des erreurs de saturation de mémoire sur nos GPUs. Plusieurs combinaisons ont été testées sur les deux bases de données, mêlant des méthodes de rognage (découpage d'une image en 2x2 plus petites images par exemple ; ce qui peut couper de gros objets et les rendre plus difficiles à apprendre) et de redimension (réduire la taille d'une image par deux  ; ce qui entraîne une baisse de sa résolution altérant, là aussi, l'apprentissage). Ces expériences nous ont permis de déterminer la meilleure résolution considérant des tailles d'images acceptables pour nos deux bases de données. Quelques expériences complémentaires ont également éte réalisées pour tester l'efficacité d'un rognage aléatoire (et non plus \textit{ad hoc}), et il s'est avéré que cela produisait effectivement de meilleurs résultats, permettant à notre architecture de mieux comprendre le contexte dans lequel différentes classes peuvent apparaître.
\item Nous avons ensuite testé chacune de nos architectures sur le \textit{Swiss dataset}, afin de cibler laquelle était la plus efficace pour des prises de vues aériennes. Nous avions donc 10 architectures différentes (SegNet, FCN-32,-16,-8, FCN-32-ResNet-50,-101,-152, and FCN-16-ResNet-50,-101,-152). Il est apparu que l'architecture FCN-16-ResNet-152 présente les meilleurs résultats du point de vue de chacune de nos métriques, et que FCN-16-ResNet-50 (sa variante contenant seulement 50 layers) est une bonne alternative, car permet de traiter de plus grandes images. Ces deux architectures seront donc retenues pour les prochaines expériences.\todo{check results}
\item Un autre point à considérer est la taille de nos bases de données ; celles-ci contiennent respectivement 100 et 28 images pour le \textit{Swiss dataset} et pour le \textit{Okutama dataset}, ce qui est très peu. Nous essayons alors d'augmenter légèrement les performances de notre architecture en utilisant des méthodes pour augmenter le nombre d'images, supposant que cela permettra un meilleur apprentissage. Les trois méthodes utilisées sont le "miroir" (consistant à retourner horizontalement ou verticalement une image), la "pixelisation" (consistant à prendre un pixel au hasard sur de petites fenêtres de l'image, et à le répartir sur l'ensemble de cette fenêtre) et l'ajout de bruit (bruitage de l'image). Nos expériences n'ont montré d'améliorations que pour le miroir qui est, en réalité, la seule méthode à ne pas altérer l'image. La pixelisation et le bruitage pourrait éventuellement servir pour des images provenant de caméras avec différentes caractéristiques (moins bonne résolution par exemple).\todo{check results}
\item Qui plus est, nous nous sommes également intéressés à la flexibilité de nos apprentissages. Effectivement, il est intéressant de savoir si un apprentissage sur le \textit{Swiss dataset} est efficace sur les images prises à Okutama, ou même si l'on peut améliorer des performances observées sur l'une des base de données en s'aidant de l'autre. Pour cela, nous avons utilisé une méthode de \textit{Deep Transfer Learning}, le multi-source, cherchant à améliorer les performances d'un apprentissage (effectué à partir d'une source $S_1$) en s'aidant d'une autre source (une autre base de données $S_2$ par exemple). Les expériences montrent globalement une légère amélioration des performances sur Okutama sur un apprentissage réalisé sur \textit{Okutama dataset} lorsque nous le complétons avec le \textit{Swiss dataset}, et une amélioration globale des peformances sur les deux bases de données. Cela montre qu'il est possible de conserver la performance d'un apprentissage tout en y intégrant un autre, mais dans une moindre mesure.\todo{check results}
\item Cette dernière partie visait à tester la méthode des ensembles, introduite pqr Dietterich~\cite{DIET00} consistant à faire une moyenne des résultats de plusieurs apprentissages distincts, de façon à améliorer la précision finale. Cette technique s'est révélée très efficace, mais également très coûteuse en temps (nécessite de trouver les résultats de chaque architecture avant de faire leur moyenne). Pour palier à ce problème, nous avons également décidé d'appliquer la méthode de Hinton et al.~\cite{HINT15}, permettant de compresser cet ensemble d'architectures en une seule, accélérant considérablement la vitesse de traitement. L'application de cette méthode de compression a légèrement réduit les bons résultats que nous avions, mais a considérablement réduit le temps nécessaire pour traiter une image, permettant à nouveau un traitement en temps presque réel.\todo{check results}
\end{enumerate}

Finalement, l'architecture que nous avons créé utilisant la structure de ResNet et la méthode d'\textit{upsampling} de FCN-16 pour la segmentation s'est avérée être la plus efficace pour nos prises de vues aériennes. En ajoutant les méthodes d'augmentation de données, et en utilisant la méthode des ensembles compressés, nous obtenons des résultats très largement exploitables et implémentables sur un drone.

